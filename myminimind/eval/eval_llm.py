import time
import torch

from typing import Any

from transformers import AutoTokenizer, TextStreamer, BatchEncoding
from myminimind.config import get_infer_config
from myminimind.config.schema import InferConfig
from myminimind.model.minimind_config import MiniMindConfig
from myminimind.model.minimind_model import MiniMindForCausalLM
from myminimind.utils.train_utils import get_model_params, setup_seed

def init_model(infer_cfg: InferConfig) -> tuple[MiniMindForCausalLM, Any]:
    tokenizer = AutoTokenizer.from_pretrained(infer_cfg.tokenizer_path)
    model_config = MiniMindConfig(**infer_cfg.to_lm_config_kwargs())
    model = MiniMindForCausalLM(model_config)
    moe_suffix = "_moe" if infer_cfg.use_moe else ""
    ckpt = f"./{infer_cfg.save_dir}/{infer_cfg.weight}_{infer_cfg.hidden_size}{moe_suffix}.pth"
    model.load_state_dict(torch.load(ckpt, map_location=infer_cfg.device), strict=True)
    
    get_model_params(model, model_config)
    model = model.to(infer_cfg.device)

    return model, tokenizer


def main():
    infer_cfg = get_infer_config()
    prompts = [
        'ä½ æœ‰ä»€ä¹ˆç‰¹é•¿ï¼Ÿ',
        'ä¸ºä»€ä¹ˆå¤©ç©ºæ˜¯è“è‰²çš„',
        'è¯·ç”¨Pythonå†™ä¸€ä¸ªè®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—çš„å‡½æ•°',
        'è§£é‡Šä¸€ä¸‹"å…‰åˆä½œç”¨"çš„åŸºæœ¬è¿‡ç¨‹',
        'å¦‚æœæ˜å¤©ä¸‹é›¨ï¼Œæˆ‘åº”è¯¥å¦‚ä½•å‡ºé—¨',
        'æ¯”è¾ƒä¸€ä¸‹çŒ«å’Œç‹—ä½œä¸ºå® ç‰©çš„ä¼˜ç¼ºç‚¹',
        'è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ',
        'æ¨èä¸€äº›ä¸­å›½çš„ç¾é£Ÿ'
    ]

    conversation = []
    model, tokenizer = init_model(infer_cfg)
    input_mode = int(input('[0] è‡ªåŠ¨æµ‹è¯•\n[1] æ‰‹åŠ¨è¾“å…¥\n'))
    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)

    prompt_iter = prompts if input_mode == 0 else iter(lambda: input('ğŸ’¬: '), '')
    for prompt in prompt_iter:
        setup_seed(2026)
        if input_mode == 0:
            print(f'ğŸ’¬: {prompt}')
        conversation = conversation[-infer_cfg.historys:] if infer_cfg.historys > 0 else []
        conversation.append({"role": "user", "content": prompt})

        templates = {"conversation": conversation, "tokenize": False, "add_generation_prompt": True}
        if infer_cfg.weight == "reason":
            # å¯ç”¨æ€è€ƒæ¨¡å¼
            templates["enable_thinking"] = True
        
        inputs = tokenizer.apply_chat_template(**templates) if infer_cfg.weight != "pretrain" else (tokenizer.bos_token + prompt)
        inputs: BatchEncoding = tokenizer(inputs, return_tensors="pt", truncation=True)
        inputs = inputs.to(infer_cfg.device)

        print('ğŸ¤–: ', end='')
        start_time = time.time()
        
        generated_ids = model.generate(
            inputs=inputs["input_ids"],
            attention_mask=inputs["attention_mask"],
            max_new_tokens=infer_cfg.max_new_tokens,
            do_sample=True,
            streamer=streamer,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id,
            top_p=infer_cfg.top_p,
            temperature=infer_cfg.temperature,
            repetition_penalty=1.0,
        )

        response = tokenizer.decode(generated_ids[0][len(inputs["input_ids"][0]):], skip_special_tokens=True)
        conversation.append({"role": "assistant", "content": response})
        gen_tokens = len(generated_ids[0][len(inputs["input_ids"][0]):])
        print(f'\n[Speed]: {gen_tokens / (time.time() - start_time):.2f} tokens/s\n\n') if infer_cfg.show_speed else print('\n\n')


if __name__ == "__main__":
    main()